#!/usr/bin/env bash
# NPM Backup Script
# Creates timestamped backups of NPM data and certificates, optionally uploads to S3,
# and manages local retention policy.

set -euo pipefail

# Capture start time for duration tracking
START_TIME=$(date +%s)

# State directory for backup status (sentinel files)
STATE_DIR="/var/lib/northstar/npm"
SENTINEL_LAST_RUN="${STATE_DIR}/backup-last-run"
SENTINEL_LAST_SUCCESS="${STATE_DIR}/backup-last-success"
SENTINEL_LAST_FAILURE="${STATE_DIR}/backup-last-failure"

# Ensure state directory exists with safe permissions
mkdir -p "$STATE_DIR"
chmod 0750 "$STATE_DIR"
chown root:root "$STATE_DIR"

# Helper function to write sentinel files
write_sentinel() {
    local file="$1"
    local content="$2"
    echo "$content" > "$file"
    chmod 0640 "$file"
}

# Helper function to emit structured log and exit
emit_log_and_exit() {
    local status="$1"
    local detail="$2"
    local exit_code="$3"
    local end_time=$(date +%s)
    local duration=$((end_time - START_TIME))
    
    if [[ "$status" == "success" ]]; then
        echo "NORTHSTAR_BACKUP status=success path=${detail} duration_s=${duration}"
    else
        echo "NORTHSTAR_BACKUP status=failure reason=${detail} duration_s=${duration}"
    fi
    
    exit "$exit_code"
}

# Helper function to record failure and exit
fail_backup() {
    local reason="$1"
    local timestamp=$(date -Iseconds)
    write_sentinel "$SENTINEL_LAST_FAILURE" "${timestamp} ${reason}"
    echo "Error: $reason"
    emit_log_and_exit "failure" "$reason" 1
}

# Record backup run start
write_sentinel "$SENTINEL_LAST_RUN" "$(date -Iseconds)"

# Prevent concurrent runs (timer + manual overlap)
LOCK_DIR="/var/lock"
LOCK_FILE="${LOCK_DIR}/npm-backup.lock"

# Ensure lock directory exists (normally present, but be safe)
[[ -d "$LOCK_DIR" ]] || mkdir -p "$LOCK_DIR"

# Acquire exclusive lock (non-blocking)
exec 200>"${LOCK_FILE}"
if ! flock -n 200; then
    fail_backup "concurrent_run_in_progress"
fi

# Configuration file path
CONFIG_FILE="/etc/npm-backup.conf"

# Default values
LOCAL_BACKUP_DIR="/var/backups"
S3_BUCKET=""
S3_PREFIX="npm"
LOCAL_RETENTION=7

# Parse configuration file
if [[ -f "$CONFIG_FILE" ]]; then
    # Read the [backup] section
    in_section=false
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Remove leading/trailing whitespace
        line=$(echo "$line" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
        
        # Skip empty lines and comments
        [[ -z "$line" || "$line" =~ ^# ]] && continue
        
        # Check for section header
        if [[ "$line" == "[backup]" ]]; then
            in_section=true
            continue
        fi
        
        # If we're in the backup section, parse key=value pairs
        if [[ "$in_section" == true ]]; then
            # Stop at next section
            [[ "$line" =~ ^\[.*\]$ ]] && break
            
            # Extract key and value
            if [[ "$line" =~ ^([^=]+)=(.*)$ ]]; then
                key=$(echo "${BASH_REMATCH[1]}" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
                value=$(echo "${BASH_REMATCH[2]}" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
                
                case "$key" in
                    local_backup_dir)
                        LOCAL_BACKUP_DIR="$value"
                        ;;
                    s3_bucket)
                        S3_BUCKET="$value"
                        ;;
                    s3_prefix)
                        S3_PREFIX="$value"
                        ;;
                    local_retention)
                        LOCAL_RETENTION="$value"
                        ;;
                esac
            fi
        fi
    done < "$CONFIG_FILE"
fi

# Validate local_retention to prevent unbounded disk growth
if [[ "$LOCAL_RETENTION" -lt 1 ]]; then
    fail_backup "invalid_retention_value"
fi

# Ensure local_backup_dir exists
if ! mkdir -p "$LOCAL_BACKUP_DIR" 2>/dev/null; then
    fail_backup "cannot_create_backup_dir"
fi

# Check if backup directory is writable
if [[ ! -w "$LOCAL_BACKUP_DIR" ]]; then
    fail_backup "backup_dir_not_writable"
fi

# Generate timestamp for backup filename
TIMESTAMP=$(date +%Y%m%d%H%M%S)
BACKUP_FILENAME="npm-${TIMESTAMP}.tar.gz"
BACKUP_PATH="${LOCAL_BACKUP_DIR}/${BACKUP_FILENAME}"

# Data directories to backup
DATA_DIR="/opt/npm/data"
LETSENCRYPT_DIR="/opt/npm/letsencrypt"

echo "Starting NPM backup..."
echo "Backup directory: $LOCAL_BACKUP_DIR"
echo "Creating backup: $BACKUP_FILENAME"

# Verify source directories exist
if [[ ! -d "$DATA_DIR" ]]; then
    echo "Warning: Data directory $DATA_DIR does not exist"
fi

if [[ ! -d "$LETSENCRYPT_DIR" ]]; then
    echo "Warning: Let's Encrypt directory $LETSENCRYPT_DIR does not exist"
fi

# Create backup archive
# Use tar from root (/) so absolute paths are preserved for restore
cd /
if ! tar -czf "$BACKUP_PATH" \
    "$DATA_DIR" \
    "$LETSENCRYPT_DIR" 2>/dev/null; then
    fail_backup "tar_archive_failed"
fi

# Verify backup was created
if [[ ! -f "$BACKUP_PATH" ]]; then
    fail_backup "backup_file_not_created"
fi

BACKUP_SIZE=$(du -h "$BACKUP_PATH" | cut -f1)
echo "Backup created successfully: $BACKUP_PATH (size: $BACKUP_SIZE)"

# Upload to S3 if configured (warning on failure, does not fail backup)
S3_UPLOAD_STATUS="skipped"
if [[ -n "$S3_BUCKET" ]]; then
    echo "Attempting S3 upload..."
    
    # Check if aws CLI is available
    if ! command -v aws &> /dev/null; then
        echo "Warning: AWS CLI not found, skipping S3 upload"
        S3_UPLOAD_STATUS="aws_cli_missing"
    else
        # Build S3 key (do not log full path for security)
        if [[ -n "$S3_PREFIX" ]]; then
            S3_KEY="s3://${S3_BUCKET}/${S3_PREFIX}/${BACKUP_FILENAME}"
        else
            S3_KEY="s3://${S3_BUCKET}/${BACKUP_FILENAME}"
        fi
        
        echo "Uploading to S3..."
        
        # Attempt upload (don't fail script if this fails)
        if aws s3 cp "$BACKUP_PATH" "$S3_KEY" --only-show-errors 2>&1; then
            echo "S3 upload completed successfully"
            S3_UPLOAD_STATUS="success"
        else
            echo "Warning: S3 upload failed, but local backup was created successfully"
            echo "Local backup is available at: $BACKUP_PATH"
            S3_UPLOAD_STATUS="failed"
        fi
    fi
else
    echo "S3 upload skipped (s3_bucket not configured)"
fi

# Implement local retention policy (LOCAL_RETENTION >= 1 enforced above)
echo "Applying retention policy (keeping $LOCAL_RETENTION most recent backups)..."

# Find all npm backup files, sorted by modification time (oldest first)
mapfile -t backup_files < <(find "$LOCAL_BACKUP_DIR" -maxdepth 1 -type f -name "npm-*.tar.gz" -printf '%T@ %p\n' | sort -n | cut -d' ' -f2-)

BACKUP_COUNT=${#backup_files[@]}

if [[ $BACKUP_COUNT -gt $LOCAL_RETENTION ]]; then
    FILES_TO_DELETE=$((BACKUP_COUNT - LOCAL_RETENTION))
    echo "Found $BACKUP_COUNT backups, deleting $FILES_TO_DELETE oldest backup(s)..."
    
    # Delete oldest files
    for ((i=0; i<FILES_TO_DELETE; i++)); do
        OLD_FILE="${backup_files[$i]}"
        echo "Deleting old backup: $(basename "$OLD_FILE")"
        rm -f "$OLD_FILE"
    done
    
    echo "Retention cleanup completed. $LOCAL_RETENTION most recent backups retained."
else
    echo "No cleanup needed ($BACKUP_COUNT backups, retention limit: $LOCAL_RETENTION)"
fi

# Record success
TIMESTAMP_ISO=$(date -Iseconds)
write_sentinel "$SENTINEL_LAST_SUCCESS" "${TIMESTAMP_ISO} ${BACKUP_FILENAME}"

# Clear any previous failure (overwrite with empty or remove)
if [[ -f "$SENTINEL_LAST_FAILURE" ]]; then
    rm -f "$SENTINEL_LAST_FAILURE"
fi

echo "Backup completed successfully"
emit_log_and_exit "success" "$BACKUP_PATH" 0
